{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HERSH DHILLON - lab_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox5bZYB3DNOC",
        "colab_type": "text"
      },
      "source": [
        "# **Lab 2**\n",
        "### Weightage 3%\n",
        "## Linear Regression\n",
        "---\n",
        "Dataset used: \n",
        "  ACS Flammability Limit Computation\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Maximum Points in the Lab: 65\n",
        "\n",
        "---\n",
        "Important points to remember :\n",
        "\n",
        "\n",
        "1.  Observations for the experiments should be explained.\n",
        "2. All the code should be submitted in the form of a single Jupyter notebook itself.\n",
        "3. Points for each sub-section are mentioned in the appropriate question.\n",
        "4. Make sure to begin early as a few experiments may consume more time to run.\n",
        "5. You can use Google colab to run in jupyter notebook (https://colab.research.google.com/) How to load data in Google Colab ?(https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)\n",
        "6. The lab must be submitted on Google classroom. The code as well as the accompanying observations should be made part of the python notebook.\n",
        "7. **Code Readability** is very important. Hence use self explanatory variable names and add comments to describe your approach wherever necessary.\n",
        "8. You are expected to submit your **detailed inferences** and not just an error free code.\n",
        "9. The lab is due on **Feb 21st 11.59pm**.\n",
        "10. The lab should be completed **individually**. Students are expected to follow the **honor code** of the class.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvZ4obBVCkdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-AitLgoFqGc",
        "colab_type": "text"
      },
      "source": [
        "Describe the arguments and output of the linear regression function call in sklearn package.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**5 Points** \n",
        "\n",
        "\n",
        "---\n",
        "**ATTRIBUTES**\n",
        "\n",
        "**fit_intercept :-** (bool default True)\n",
        "Whether or not to calculate the intercept for this model. If set to False, intercept will be taken as 0.\n",
        "\n",
        "**normalize :-** (bool default False)\n",
        "If True, X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
        "\n",
        "**copy_X :-** (bool default True)\n",
        "If True, X will be copied; else, it may be overwritten.\n",
        "\n",
        "**n_jobs :-** (int or None)\n",
        "The number of jobs to use for the computation.\n",
        "\n",
        "LinearRegression() returns a linear regression object according to the given parameters\n",
        "\n",
        "**METHODS**\n",
        "\n",
        "**fit(self, X, y, sample_weight=None) :-** Fits the linear model with the help of the data provided.\n",
        "\n",
        "**predict(self, X) :-** Gives predictions for X provided. Returns an array of prediction values\n",
        "\n",
        "**score(self, X, y, sample_weight=None) :-** returns the R^2 score(Float type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrzJ2WFeGFZ1",
        "colab_type": "text"
      },
      "source": [
        "We will be using the ACS flammability limit computation dataset for linear regression task. The dataset contains 105 molecular properties of few chemical compounds from which Flammability Limit is to be computed. The following block contains code to read the excel file from the given URL and preprocess the data. **Please don't make any changes to the block below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxOnNEVUG0Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the dataset onto a data frame. !! Please dont make any changes !!\n",
        "url = \"https://ndownloader.figshare.com/files/18729491\"\n",
        "data_frame = pd.read_excel(url,encoding='latin-1',skiprows=5)\n",
        "\n",
        "# Necessary attributes converted to numpy arrays\n",
        "target = data_frame.iloc[:,4].to_numpy()\n",
        "input_features = data_frame.iloc[:,6:].to_numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSmvKwKtS0eb",
        "colab_type": "text"
      },
      "source": [
        "We have input attributes and output vector ready. Now the first step is to split the entire dataset into training and test subsets. sklearn package provides you a flexible way of doing this. All you need to do is just specify input attributes array, target array and fraction of the test set. Once the split is done, we are ready to train the model and evaluate it. \n",
        "\n",
        "- We would like you to experiment with different test set fractions and comment on how well the model is able to estimate the target value for new test instances? \n",
        "\n",
        "- Also there is another flexibility to tune the intercept term on or off. We would like you to experiment on those possibilities as well.\n",
        "\n",
        "**Note:** Don't forget to explicitly normalize the attributes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**10 Points** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZUtbQVUldbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalizing the data set\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "y = target.reshape((-1,1))\n",
        "\n",
        "X_normalized_features = normalize(input_features, axis = 0)\n",
        "X_normalized_samples = normalize(input_features, axis = 1)\n",
        "y_normalized = normalize(y, axis = 0)\n",
        "\n",
        "y_max = max(y)\n",
        "y_min = min(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzyj0Ia3V166",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e6c9b8ad-bb1e-468b-992e-cd875dcc0ffb"
      },
      "source": [
        "# 1. Using the original input_variables and target values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "test_size_fractions = [0.2,0.3,0.4,0.5]\n",
        "\n",
        "mse_list_original = []\n",
        "\n",
        "# Step 1: Create a linear regressor object \n",
        "# Insert your code below\n",
        "regressor_with_intercept = LinearRegression()\n",
        "\n",
        "# Step 2: Create a linear regressor object forcing bias or intercept as 0\n",
        "# Insert your code below\n",
        "regressor_no_intercept = LinearRegression(fit_intercept = False)\n",
        "\n",
        "# Experiment with 8 different model setup\n",
        "\n",
        "for fraction in test_size_fractions:\n",
        "    \n",
        "    \n",
        "    # Step 3: Split into train and test set based on the fraction\n",
        "    # Insert your code below\n",
        "    X_train, X_test, y_train, y_test = train_test_split(input_features, target, test_size = fraction, random_state = 42)\n",
        "\n",
        "    # Step 4(a): Train the Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_with_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4(b): Predict the target values using the trained model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_1 = regressor_with_intercept.predict(X_test)\n",
        "\n",
        "    # Step 4(c): Print the mean squared error of the model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_1 = mean_squared_error(y_test, y_test_predictions_1)\n",
        "    mse_list_original.append(final_loss_1)\n",
        "    print('With intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_1))\n",
        "\n",
        "    # Step 5(a): Train the Non Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_no_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5(b): Predict the target values using the trained non intercept for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_2 = regressor_no_intercept.predict(X_test)\n",
        "\n",
        "    # Step 5(c): Print the mean squared error of the non intercept model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_2 = mean_squared_error(y_test, y_test_predictions_2)\n",
        "    mse_list_original.append(final_loss_2)\n",
        "    print('Without intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With intercept, fraction 0.2 MSE = 0.0858292140362414\n",
            "Without intercept, fraction 0.2 MSE = 0.18274993369871623\n",
            "With intercept, fraction 0.3 MSE = 0.10516608109969937\n",
            "Without intercept, fraction 0.3 MSE = 0.22601866092166925\n",
            "With intercept, fraction 0.4 MSE = 0.15767198520486844\n",
            "Without intercept, fraction 0.4 MSE = 0.38801607113411896\n",
            "With intercept, fraction 0.5 MSE = 0.12733412702050131\n",
            "Without intercept, fraction 0.5 MSE = 0.3176483326334792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KaulLF8LPuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "138ec4a8-e993-4e5c-c39c-86c2e2406440"
      },
      "source": [
        "# 2. Using the normalized input on features and output values\n",
        "\n",
        "test_size_fractions = [0.2,0.3,0.4,0.5]\n",
        "\n",
        "mse_list_normal_features = []\n",
        "\n",
        "# Step 1: Create a linear regressor object \n",
        "# Insert your code below\n",
        "regressor_with_intercept = LinearRegression()\n",
        "\n",
        "# Step 2: Create a linear regressor object forcing bias or intercept as 0\n",
        "# Insert your code below\n",
        "regressor_no_intercept = LinearRegression(fit_intercept = False)\n",
        "\n",
        "# Experiment with 8 different model setup\n",
        "\n",
        "for fraction in test_size_fractions:\n",
        "    \n",
        "    \n",
        "    # Step 3: Split into train and test set based on the fraction\n",
        "    # Insert your code below\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_normalized_features, target, test_size = fraction, random_state = 42)\n",
        "\n",
        "\n",
        "    # Step 4(a): Train the Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_with_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4(b): Predict the target values using the trained model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_1 = regressor_with_intercept.predict(X_test)\n",
        "\n",
        "    # Step 4(c): Print the mean squared error of the model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_1 = mean_squared_error(y_test, y_test_predictions_1)\n",
        "    mse_list_normal_features.append(final_loss_1)\n",
        "    print('With intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_1))\n",
        "\n",
        "    # Step 5(a): Train the Non Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_no_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5(b): Predict the target values using the trained non intercept for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_2 = regressor_no_intercept.predict(X_test)\n",
        "\n",
        "    # Step 5(c): Print the mean squared error of the non intercept model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_2 = mean_squared_error(y_test, y_test_predictions_2)\n",
        "    mse_list_normal_features.append(final_loss_2)\n",
        "    print('Without intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With intercept, fraction 0.2 MSE = 1.6086939175280427e+24\n",
            "Without intercept, fraction 0.2 MSE = 6.854546440543806e+22\n",
            "With intercept, fraction 0.3 MSE = 9.489015270014045e+21\n",
            "Without intercept, fraction 0.3 MSE = 8.727066358649256e+24\n",
            "With intercept, fraction 0.4 MSE = 3.2702226671685005e+24\n",
            "Without intercept, fraction 0.4 MSE = 6.527728997020021e+24\n",
            "With intercept, fraction 0.5 MSE = 6.098971594530579e+24\n",
            "Without intercept, fraction 0.5 MSE = 9.910156024630938e+21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DriMG_ljLP9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "953106a6-2ed1-495b-b572-e490c6593602"
      },
      "source": [
        "# 3. Using the Normalized features on samples and target values\n",
        "\n",
        "test_size_fractions = [0.2,0.3,0.4,0.5]\n",
        "\n",
        "mse_list_normal_samples = []\n",
        "\n",
        "# Step 1: Create a linear regressor object \n",
        "# Insert your code below\n",
        "regressor_with_intercept = LinearRegression()\n",
        "\n",
        "# Step 2: Create a linear regressor object forcing bias or intercept as 0\n",
        "# Insert your code below\n",
        "regressor_no_intercept = LinearRegression(fit_intercept = False)\n",
        "\n",
        "# Experiment with 8 different model setup\n",
        "\n",
        "for fraction in test_size_fractions:\n",
        "    \n",
        "    \n",
        "    # Step 3: Split into train and test set based on the fraction\n",
        "    # Insert your code below\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_normalized_samples, target, test_size = fraction, random_state = 42)\n",
        "\n",
        "\n",
        "    # Step 4(a): Train the Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_with_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4(b): Predict the target values using the trained model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_1 = regressor_with_intercept.predict(X_test)\n",
        "\n",
        "    # Step 4(c): Print the mean squared error of the model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_1 = mean_squared_error(y_test, y_test_predictions_1)\n",
        "    mse_list_normal_samples.append(final_loss_1)\n",
        "    print('With intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_1))\n",
        "\n",
        "    # Step 5(a): Train the Non Intercept Linear Regression model using held out training set\n",
        "    # Insert your code below\n",
        "    regressor_no_intercept.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5(b): Predict the target values using the trained non intercept for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions_2 = regressor_no_intercept.predict(X_test)\n",
        "\n",
        "    # Step 5(c): Print the mean squared error of the non intercept model on test data set. Store this value to mse list to find the best model setting.\n",
        "    # Insert your code below\n",
        "    final_loss_2 = mean_squared_error(y_test, y_test_predictions_2)\n",
        "    mse_list_normal_samples.append(final_loss_2)\n",
        "    print('Without intercept, fraction ' + str(fraction) + ' MSE = ' + str(final_loss_2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With intercept, fraction 0.2 MSE = 3.964918025922237e+20\n",
            "Without intercept, fraction 0.2 MSE = 0.07303269019177201\n",
            "With intercept, fraction 0.3 MSE = 4.498524370896273e+20\n",
            "Without intercept, fraction 0.3 MSE = 0.07677604148719748\n",
            "With intercept, fraction 0.4 MSE = 0.09571231140037624\n",
            "Without intercept, fraction 0.4 MSE = 0.11009041444307918\n",
            "With intercept, fraction 0.5 MSE = 6.724576667732859e+19\n",
            "Without intercept, fraction 0.5 MSE = 0.16195835624983348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8aRGM_afDmY",
        "colab_type": "text"
      },
      "source": [
        "In the previous block you have experimented with different settings of the Linear Regression model. You have test set mean squared error stored. Figure out which setting gave the least mean squared error. \n",
        "\n",
        "---\n",
        "\n",
        "**5 Points**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyFU-GFUhZkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "13f465ce-3e83-40cf-927e-fdc1b31d6a4e"
      },
      "source": [
        "# Insert your code to identify the best fraction and intercept requirement. Also, print the best setting chosen.\n",
        "\n",
        "# 1. Checking the best normalization setting\n",
        "\n",
        "print('Mean MSE original values ', np.mean(mse_list_original))\n",
        "print('Mean MSE normalized feature values ', np.mean(mse_list_normal_features))\n",
        "print('Mean MSE normalized samples values ', np.mean(mse_list_normal_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean MSE original values  0.19880430071866179\n",
            "Mean MSE normalized feature values  3.29007852132456e+24\n",
            "Mean MSE normalized samples values  1.1419875079489744e+20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIrq3ve7n36x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0150c66e-f1cc-4a6b-c470-38209b702c9c"
      },
      "source": [
        "# 2. Best fraction and intercept requirement\n",
        "\n",
        "mse_min = min(mse_list_original)\n",
        "\n",
        "ind = mse_list_original.index(mse_min)\n",
        "print(ind)\n",
        "final_intercept = False\n",
        "\n",
        "final_fraction = 0.2\n",
        "\n",
        "if ind%2 == 0:\n",
        "    final_intercept = True\n",
        "\n",
        "if ind/2 == 1:\n",
        "    final_fraction = 0.3\n",
        "elif ind/2 == 2:\n",
        "    final_fraction = 0.4\n",
        "elif ind/2 == 3:\n",
        "    final_fraction = 0.5\n",
        "\n",
        "print(final_intercept)\n",
        "print(final_fraction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "True\n",
            "0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rp9zsZe9PQu",
        "colab_type": "text"
      },
      "source": [
        "**Reason for the best model**\n",
        "\n",
        "The best model was found to be the one with original values with 0.2 train_test_split with intercept calculation. The fraction value and the intercept status were expected as with more training data we can model the same dataset more accurately, leading to less errors. Also since we have not standardized and the data is not centered, it intercept calculation seems a better option.\n",
        "\n",
        "The unexpected part is that the original values give the best output. It seems as if the normalized data do not capture the relationship in the data that is present in the original data. Normalization across a sample performs better than across a feature. The feature normalized values might mot be able to capture the relationship in one input vector \n",
        "\n",
        "The fit with the original values has been taken as the best although one setting of the normalization across samples gives a lower MSE, there still exist settings in which the error exceeds 10^20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQVehqr9hl6x",
        "colab_type": "text"
      },
      "source": [
        "Now that you have figured out the best model setting, use that setting to train the model to predict test instances.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**3 Points**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJEf7Rg8h7oS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00bc8df8-f2f5-4f13-9fe5-78cb29ba3586"
      },
      "source": [
        "# Step 1: Split the entire dataset into train and test set based on the best fraction\n",
        "# Insert your code below\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_features, target, test_size = final_fraction, random_state = 42)\n",
        "\n",
        "# Step 2: Create a linear regressor object with intercept turned on or off as per best model setting\n",
        "# Insert your code below\n",
        "regressor_with_intercept = LinearRegression(fit_intercept=final_intercept)\n",
        "\n",
        "# Step 3(a): Train the Linear Regression model using training set\n",
        "# Insert your code below\n",
        "regressor_with_intercept.fit(X_train, y_train)\n",
        "\n",
        "# Step 3(b): Predict the target values using the trained model for test set instances\n",
        "# Insert your code below\n",
        "y_test_predictions = regressor_with_intercept.predict(X_test)\n",
        "\n",
        "# Step 3(c): Print the mean squared error of the model on test set. \n",
        "# Insert your code below\n",
        "final_loss = mean_squared_error(y_test, y_test_predictions)\n",
        "print(final_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0858292140362414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHT58sjij0f4",
        "colab_type": "text"
      },
      "source": [
        "Now check the goodness of the model with repsect to prediction on test instances. The aim is to predict the target values for test instances using the trained model. These values would be plotted against the true values of the test instances. A good model would yield a plot that looks *close to a line of slope 45 degree*. **Comment** on the goodness of the learned model using this test.\n",
        "\n",
        "---\n",
        "\n",
        "**5 points** \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDziN7WlJHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "aa23c9b0-7038-440a-a434-bfca417a64cd"
      },
      "source": [
        "# Insert your code to plot true vs predicted test values\n",
        "line = [-2,-1, 1, 2, 3]  # for plotting the 45 degree line\n",
        "\n",
        "plt.scatter(y_test, y_test_predictions, s = np.pi*1 )\n",
        "plt.xlim(-1, 3)\n",
        "plt.ylim(-1, 3)\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.plot(line, line, color = 'red')\n",
        "plt.title('True v tested predictions')\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Prediction')\n",
        "plt.show\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEWCAYAAAC9hIj8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD/B3AdDfju3AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEzwhuUH9k5t",
        "colab_type": "text"
      },
      "source": [
        "**COMMENT ON THE GOODNESS**\n",
        "\n",
        "Although the data seems curved than a 45 degree line, however the points do seem close to that line. Apart from a few outliers, the true and predicted labels are pretty close to each other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6jQmj8amZTr",
        "colab_type": "text"
      },
      "source": [
        "You have implemented simple linear regression. You may apply regularization techniques that constrain the coefficients. We will be seeing 2 types of regularized regression techniques:\n",
        "\n",
        "\n",
        "1.   Ridge Regression\n",
        "2.   Lasso Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sxHV6iP4Mlwg"
      },
      "source": [
        "# Ridge Regression\n",
        "\n",
        "---\n",
        "\n",
        "Describe the arguments and output of the Ridge regression function call in sklearn package.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**5 Points** \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**PARAMETERS**\n",
        "\n",
        "**alpha(float, default=1.0) :-**\n",
        "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates.\n",
        "\n",
        "**fit_intercept(bool, default=True):-**\n",
        "Whether to calculate the intercept for this model. If set to false, no intercept willbe used in calculations.\n",
        "\n",
        "**normalize (bool, default=False):-**\n",
        "If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
        "\n",
        "**copy_X (bool, default=True) :-**\n",
        "If True, X will be copied; else, it may be overwritten.\n",
        "\n",
        "**max_iter (int, default=None) :-**\n",
        "Maximum number of iterations for conjugate gradient solver.\n",
        "\n",
        "**tol (float, default=1e-3) :-**\n",
        "Tolerance for the precision of the solution.\n",
        "\n",
        "**solver (default='auto') :-**\n",
        "Solver to use in the computational routines.\n",
        "\n",
        "**random_state (int, RandomState instance, default=None) :-**\n",
        "The seed of the pseudo random number generator to use when shuffling the data.\n",
        "\n",
        "Returns a ridge regressor object.\n",
        "\n",
        "**METHODS**\n",
        "\n",
        "**fit(self, X, y, sample_weight=None) :-** Fits the linear model with the help of the data provided.\n",
        "\n",
        "**predict(self, X) :-** Gives predictions for X provided. Returns an array of prediction values\n",
        "\n",
        "**score(self, X, y, sample_weight=None) :-** returns the R^2 score(Float type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wyr-kOLm-AT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In ridge regression, L-2 regularization is applied to the coefficients. You have a hyperparameter alpha which you can vary to control the extent of constraining the coefficients. We would like you to experiment with different values of alpha and comment on the model performance. You may use the best setting of Linear Regression to Ridge Regression also. **Comment** on the effect of the alpha value on the mean squared error.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**9 Points**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2MHw9z8oKVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "4afe004d-96a7-445d-9474-917c9821960a"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alpha_values = [0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2]\n",
        "error_list = []\n",
        "\n",
        "# Step 1: Split into train and test set based on the best fraction\n",
        "# Insert your code below\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_features, target, test_size = final_fraction, random_state = 42)\n",
        "\n",
        "for alpha_ in alpha_values:\n",
        "\n",
        "\n",
        "    # Step 2: Create a Ridge regressor object with desired alpha value\n",
        "    # Insert your code below\n",
        "    ridge_regressor = Ridge(alpha = alpha_, fit_intercept=final_intercept)\n",
        "\n",
        "    # Step 3(a): Train the Ridge Regression model using training set\n",
        "    # Insert your code below\n",
        "    ridge_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Step 3(b): Predict the target values using the trained Ridge model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions = ridge_regressor.predict(X_test)\n",
        "\n",
        "    # Step 3(c): Print the mean squared error of the model on test set. Add it to an error list to plot later.\n",
        "    # Insert your code below\n",
        "    final_loss_ridge = mean_squared_error(y_test, y_test_predictions)\n",
        "    print(final_loss_ridge)\n",
        "    error_list.append(final_loss_ridge)\n",
        "\n",
        "# Step 4: Plot a graph of different alpha values of ridge regression versus mean squared error\n",
        "\n",
        "plt.plot(alpha_values, error_list)\n",
        "plt.title('Ridge, loss change with alpha values')\n",
        "plt.xlabel('Alpha Values')\n",
        "plt.ylabel('Error Values')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08582595578504242\n",
            "0.08583997985292101\n",
            "0.08577324026507387\n",
            "0.08547643328686323\n",
            "0.08507780120002198\n",
            "0.08480642368078638\n",
            "0.08461906728653223\n",
            "0.08448827044085874\n",
            "0.08439713847193056\n",
            "0.08433476798970439\n",
            "0.08429379317906875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImyzC-vID8ge",
        "colab_type": "text"
      },
      "source": [
        "**OBSERVATIONS** :- \n",
        "With an increasing value of alpha the error value drops. However the difference in the drop becomes smaller each time. This is expected as Ridge regression reduce the extent of overfitting by regularization. However, there can be a value for which the loss starts increasing again, as a result of penalizing the weights too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNlHlQBPNXLM",
        "colab_type": "text"
      },
      "source": [
        "# Lasso Regression\n",
        "\n",
        "---\n",
        "\n",
        "Describe the arguments and output of the Lasso regression function call in sklearn package.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**5 Points** \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**PARAMETERS**\n",
        "\n",
        "**alpha(float, default=1.0) :-**\n",
        "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates.\n",
        "\n",
        "**fit_intercept(bool, default=True):-**\n",
        "Whether to calculate the intercept for this model. If set to false, no intercept willbe used in calculations.\n",
        "\n",
        "**precompute (True False default=False) :-**\n",
        "Whether to use a precomputed Gram matrix to speed up calculations.The Gram matrix can also be passed as argument.\n",
        "\n",
        "**normalize (bool, default=False):-**\n",
        "If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
        "\n",
        "**warm_start (bool) :-**\n",
        "When set to True, reuse the solution of the previous call to fit as initialization.\n",
        "\n",
        "**positive (bool, optional) :-**\n",
        "When set to True, forces the coefficients to be positive.\n",
        "\n",
        "**copy_X (bool, default=True) :-**\n",
        "If True, X will be copied; else, it may be overwritten.\n",
        "\n",
        "**max_iter (int, default=None) :-**\n",
        "Maximum number of iterations for conjugate gradient solver.\n",
        "\n",
        "**tol (float, default=1e-3) :-**\n",
        "Tolerance for the precision of the solution.\n",
        "\n",
        "**solver (default='auto') :-**\n",
        "Solver to use in the computational routines.\n",
        "\n",
        "**random_state (int, RandomState instance, default=None) :-**\n",
        "The seed of the pseudo random number generator to use when shuffling the data.\n",
        "\n",
        "**selection (str, default cyclic) :-**\n",
        "If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default.\n",
        "\n",
        "Returns a lasso regressor object.\n",
        "\n",
        "**METHODS**\n",
        "\n",
        "**fit(self, X, y, sample_weight=None) :-** Fits the linear model with the help of the data provided.\n",
        "\n",
        "**predict(self, X) :-** Gives predictions for X provided. Returns an array of prediction values\n",
        "\n",
        "**score(self, X, y, sample_weight=None) :-** returns the R^2 score(Float type)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLj4DffxpM4K",
        "colab_type": "text"
      },
      "source": [
        "In Lasso regression, L-1 regularization is applied to the coefficients. You have a hyperparameter alpha which you can vary to control the extent of constraining the coefficients. We would like you to experiment with different values of alpha and **comment** on the model performance. You may use the best setting of Linear Regression to Lasso Regression also.\n",
        "\n",
        "---\n",
        "\n",
        "**9 Points**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyEtXEX0jZRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "f1a49bd4-25fc-43a8-d055-01889989efce"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "alpha_values = [0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2]\n",
        "error_list = []\n",
        "\n",
        "# Step 1: Split into train and test set based on the best fraction\n",
        "# Insert your code below\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_features, target, test_size = final_fraction, random_state = 42)\n",
        "\n",
        "for alpha_ in alpha_values:\n",
        "\n",
        "    # Step 2: Create a LASSO regressor object with desired alpha value\n",
        "    # Insert your code below\n",
        "    lasso_regressor = Lasso(alpha=alpha_, fit_intercept=final_intercept)\n",
        "   \n",
        "\n",
        "    # Step 3(a): Train the LASSO Regression model using training set\n",
        "    # Insert your code below\n",
        "    lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Step 3(b): Predict the target values using the trained LASSO model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions = lasso_regressor.predict(X_test)\n",
        "\n",
        "    # Step 3(c): Print the mean squared error of the model on test set. Add it to an error list to plot later.\n",
        "    # Insert your code below\n",
        "    final_loss_lasso = mean_squared_error(y_test, y_test_predictions)\n",
        "    print(final_loss_lasso)\n",
        "    error_list.append(final_loss_lasso)\n",
        "\n",
        "\n",
        "# Step 4: Plot a graph of different alpha values of lasso regression versus mean squared error\n",
        "\n",
        "plt.plot(alpha_values, error_list)\n",
        "plt.title('Lasso, loss change with alpha values')\n",
        "plt.xlabel('Alpha Values')\n",
        "plt.ylabel('Error Values')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08774810322238126\n",
            "0.08791898465277806\n",
            "0.0933842970936467\n",
            "0.10605282144112084\n",
            "0.13291101882280332\n",
            "0.1530903854529497\n",
            "0.1625139365143653\n",
            "0.16567648984513966\n",
            "0.1702554203972738\n",
            "0.17625072817076765\n",
            "0.18366241316562115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQYiISFRKECIiEpUShIiIRPX/nLJWthOEBisAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0ia3rYjGwdJ",
        "colab_type": "text"
      },
      "source": [
        "**OBSERVATIONS:-** We see an increase in the mse loss as we increase alpha for lasso regression. It might be due to the fact that LASSO also acts as a feature selector, tuning some weights to zero when there actually could have been a relationship/correlation between that feature and the output value. Hence we see that in this case Ridge performs better as it is able to capture those correlations which Lasso can't"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaY8yDgjFvwb",
        "colab_type": "text"
      },
      "source": [
        "Now since you have tried LASSO with different alpha values, you may try different number of maximum iterations associated with LASSO and **comment** on its effect on the mean squared error.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**9 Points**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51uy4oEqFuBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ed88694-5de1-40b2-fb2e-65add7572cf3"
      },
      "source": [
        "error_list = []\n",
        "alpha_ = 0.01\n",
        "iter_list = []\n",
        "\n",
        "# Step 1: Split into train and test set based on the best fraction\n",
        "# Insert your code below\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_features, target, test_size = final_fraction, random_state = 42)\n",
        "\n",
        "for m_iterations in range(1,200,2):\n",
        "    \n",
        "\n",
        "    # Step 2: Create a LASSO regressor object with desired number of iterations\n",
        "    # Insert your code below\n",
        "    lasso_regressor = Lasso(alpha=alpha_, fit_intercept=final_intercept, max_iter=m_iterations)\n",
        "\n",
        "    # Step 3(a): Train the LASSO Regression model using training set\n",
        "    # Insert your code below\n",
        "    lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Step 3(b): Predict the target values using the trained LASSO model for test set instances\n",
        "    # Insert your code below\n",
        "    y_test_predictions = lasso_regressor.predict(X_test)\n",
        "\n",
        "    # Step 3(c): Print the mean squared error of the model on test set. Add it to an error list to plot later.\n",
        "    # Insert your code below\n",
        "    final_loss_lasso = mean_squared_error(y_test, y_test_predictions)\n",
        "    error_list.append(final_loss_lasso)\n",
        "    iter_list.append(m_iterations)\n",
        "\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 4: Plot a graph of different iteration values of lasso regression versus mean squared error\n",
        "\n",
        "plt.plot(iter_list, error_list)\n",
        "plt.title('Lasso, loss change with iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error Values')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.16973021236882, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.06977290044913, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.90761531099376, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.773072749567884, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.838971964959654, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.41712339677531, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.347140253520372, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.840865188717352, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.16323682013115, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.80914644017038, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.372635967975064, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.958967795304233, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.572361846872298, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.210657094495453, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.88918878374637, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.604943324713425, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.12163041461723, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.6413890595855065, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.188797551832337, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.763600338212399, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.365051109082856, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.966471130467383, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.6100253930746575, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.257736196794504, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.930950269157229, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.628640521615104, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.348740345580346, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.08946786277388, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.849254163795699, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6266729213041273, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4204053297611807, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2292223468880294, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0519751329415854, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.887589475767122, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7350621166552003, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.593457892656545, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.461907130315012, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.339603013284062, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.225798809120022, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.119804931488247, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0209858617052348, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9287569756856868, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.842581329049743, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7619664514771571, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6864611952576638, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6156526749860873, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5491633270108736, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4866481093962491, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.427791856169037, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3723067937314894, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.319930222544734, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.270422363371594, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6197551519360474, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9853777965719459, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9606691997065866, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8637639647475481, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7643732563073513, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6750218786425819, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.596623900867101, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5280353152739039, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4679453417186181, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4151995681363587, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.367027619750246, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.32187151963246663, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28425354140471626, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25215506868423176, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22422643692719646, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1997057123493846, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17808323127508174, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15897051108768778, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14204882673807617, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12704730416109555, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.11373212841985492, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10190011760661832, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09137417927884428, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0819997208744212, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07364165015705737, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06618180717012478, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.059516745096971135, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05355580688659245, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.048219457329096826, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04343783708208093, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.039149509709417885, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03530037639863792, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.031842736130968774, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028734471829380936, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.025938345476916425, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02342138738887911, tolerance: 0.0201166147402188\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.021154366772975663, tolerance: 0.0201166147402188\n",
            "  positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhEnd27hSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6abaZ84JDCz",
        "colab_type": "text"
      },
      "source": [
        "**OBSERVATIONS:-**\n",
        "\n",
        "The best value for lasso was obtained at 0.01. Hence, that was used for this experiment. The loss initially drops as the gradient descent happens. After a point the model starts overfitting on the data so the test error swings up again. Due to regularization the curve starts decreasing once again and converges to a final value.\n",
        "\n",
        "NOTE :- We get the warnings because the model did not converge in the m_iterations allowed, which was meant to happen. Can use the ignore warning option to remove it.\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUN_9GAqxfzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}